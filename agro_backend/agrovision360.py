# -*- coding: utf-8 -*-
"""AgroVision360

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/173OMihL8bv4X5H5QS68FBUZQ02JPD_rz
"""

import tensorflow as tf

dataset_path = "/content/FINAL DATASET"

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="training",
    seed=123
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="validation",
    seed=123
)

class_names = train_ds.class_names
print(class_names)
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models

base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)

base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.4),
    layers.Dense(len(class_names), activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)
model.save("plant_disease_model.h5")
import json

with open('/content/disease_info.json') as f:
    disease_info = json.load(f)
    import numpy as np

img = tf.keras.preprocessing.image.load_img("test.jpg", target_size=(224,224))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)

prediction = model.predict(img_array)
predicted_class = class_names[np.argmax(prediction)]

print("Disease:", predicted_class)

info = disease_info[predicted_class]
print("Prevention:", info["prevention"])
print("Cure:", info["cure"])

import os
print(os.listdir('/content'))
!unzip FINAL_DATASET.zip
!unzip PREVENTIONCUREDATASET.zip
os.listdir('/content')

os.listdir('/content')

import os
os.listdir('/content')

!unzip "FINAL DATASET.zip"

import os
os.listdir('/content')

!unzip "FINAL DATASET.zip"

import os
os.listdir('/content')

!mv "FINAL DATASET" FINAL_DATASET

import os
os.listdir('/content')

dataset_path = "/content/FINAL_DATASET"

import tensorflow as tf

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="training",
    seed=123
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="validation",
    seed=123
)

class_names = train_ds.class_names
print(class_names)

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras import layers, models

base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224,224,3)
)

base_model.trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(7, activation='softmax')   # 7 crops
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

import os

dataset_path = "/content/FINAL_DATASET"

for root, dirs, files in os.walk(dataset_path):
    for file in files:
        if not file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):
            print("Non-image file found:", os.path.join(root, file))

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

from PIL import Image
import os

dataset_path = "/content/FINAL_DATASET"

bad_files = []

for root, dirs, files in os.walk(dataset_path):
    for file in files:
        file_path = os.path.join(root, file)
        try:
            with Image.open(file_path) as img:
                img.verify()   # verify image integrity
        except:
            bad_files.append(file_path)

print("Corrupted images found:")
for f in bad_files:
    print(f)

for f in bad_files:
    os.remove(f)
    print("Deleted:", f)

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="training",
    seed=123
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    image_size=(224,224),
    batch_size=32,
    validation_split=0.2,
    subset="validation",
    seed=123
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

from google.colab import files

files.download("AgroVision360")
files.download("PREVENTIONCUREDATASET.json")